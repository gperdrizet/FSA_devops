{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66b6199",
   "metadata": {},
   "source": [
    "# Lesson 29: PyTorch training loop activity - SOLUTION\n",
    "\n",
    "This notebook contains the solution for adding batching and validation to the PyTorch training loop.\n",
    "\n",
    "## Notebook set-up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(315)\n",
    "np.random.seed(315)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df12fb",
   "metadata": {},
   "source": [
    "## 1. Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('https://gperdrizet.github.io/FSA_devops/assets/data/unit4/preprocessed_housing_data.pkl')\n",
    "\n",
    "training_df = data['training_df']\n",
    "testing_df = data['testing_df']\n",
    "features = data['features']\n",
    "label = data['label']\n",
    "\n",
    "print(f'Training samples: {len(training_df)}')\n",
    "print(f'Testing samples: {len(testing_df)}')\n",
    "print(f'Features: {features}')\n",
    "print(f'Label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45497b",
   "metadata": {},
   "source": [
    "## 2. Prepare PyTorch tensors and DataLoaders - SOLUTION\n",
    "\n",
    "This section creates tensors, splits training data into train/validation sets, and creates DataLoaders for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to PyTorch tensors and move to device\n",
    "X_train_full = torch.tensor(training_df[features].values, dtype=torch.float32).to(device)\n",
    "y_train_full = torch.tensor(training_df[label].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test = torch.tensor(testing_df[features].values, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(testing_df[label].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Split training data into train and validation sets (80/20)\n",
    "n_samples = X_train_full.shape[0]\n",
    "n_val = int(n_samples * 0.2)\n",
    "n_train = n_samples - n_val\n",
    "\n",
    "# Shuffle indices for random split\n",
    "indices = torch.randperm(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "# Create train and validation tensors\n",
    "X_train = X_train_full[train_indices]\n",
    "y_train = y_train_full[train_indices]\n",
    "X_val = X_train_full[val_indices]\n",
    "y_val = y_train_full[val_indices]\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {n_train}')\n",
    "print(f'Validation samples: {n_val}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Training batches per epoch: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36ca09",
   "metadata": {},
   "source": [
    "## 3. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(32, 1)\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcac59",
   "metadata": {},
   "source": [
    "## 4. Training function - SOLUTION\n",
    "\n",
    "This solution accepts DataLoaders and iterates over batches, computing validation metrics after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38211a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epochs: int = 50,\n",
    "    print_every: int = 5\n",
    ") -> dict[str, list[float]]:\n",
    "    '''Training loop with batching and validation.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training batches\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        epochs: Number of training epochs\n",
    "        print_every: Print progress every N epochs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'loss': [], \n",
    "        'r2': [],\n",
    "        'val_loss': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Average training loss for the epoch\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Collect all training predictions for R² calculation\n",
    "            all_train_preds = []\n",
    "            all_train_labels = []\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                preds = model(X_batch)\n",
    "                all_train_preds.append(preds)\n",
    "                all_train_labels.append(y_batch)\n",
    "            \n",
    "            train_preds = torch.cat(all_train_preds)\n",
    "            train_labels = torch.cat(all_train_labels)\n",
    "            train_loss = criterion(train_preds, train_labels).item()\n",
    "            ss_res = torch.sum((train_labels - train_preds) ** 2)\n",
    "            ss_tot = torch.sum((train_labels - torch.mean(train_labels)) ** 2)\n",
    "            train_r2 = (1 - (ss_res / ss_tot)).item()\n",
    "            \n",
    "            # Collect all validation predictions\n",
    "            all_val_preds = []\n",
    "            all_val_labels = []\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                all_val_preds.append(preds)\n",
    "                all_val_labels.append(y_batch)\n",
    "            \n",
    "            val_preds = torch.cat(all_val_preds)\n",
    "            val_labels = torch.cat(all_val_labels)\n",
    "            val_loss = criterion(val_preds, val_labels).item()\n",
    "            ss_res = torch.sum((val_labels - val_preds) ** 2)\n",
    "            ss_tot = torch.sum((val_labels - torch.mean(val_labels)) ** 2)\n",
    "            val_r2 = (1 - (ss_res / ss_tot)).item()\n",
    "        \n",
    "        # Record metrics\n",
    "        history['loss'].append(train_loss)\n",
    "        history['r2'].append(train_r2)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} - '\n",
    "                  f'loss: {train_loss:.4f} - R²: {train_r2:.4f} - '\n",
    "                  f'val_loss: {val_loss:.4f} - val_R²: {val_r2:.4f}')\n",
    "    \n",
    "    print('\\nTraining complete.')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b60be0",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    print_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2abdbd",
   "metadata": {},
   "source": [
    "## 6. Learning curves\n",
    "\n",
    "Now showing both training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ddbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].plot(history['loss'], label='Training')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "\n",
    "# R² plot\n",
    "axes[1].set_title('R²')\n",
    "axes[1].plot(history['r2'], label='Training')\n",
    "axes[1].plot(history['val_r2'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('R²')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9affc",
   "metadata": {},
   "source": [
    "## 7. Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f73b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions (no gradient calculation needed)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).cpu().numpy().flatten()\n",
    "\n",
    "# Calculate R²\n",
    "ss_res = np.sum((testing_df[label].values - predictions) ** 2)\n",
    "ss_tot = np.sum((testing_df[label].values - np.mean(testing_df[label].values)) ** 2)\n",
    "rsquared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f'Model R² on test set: {rsquared:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1fb5b7",
   "metadata": {},
   "source": [
    "## 8. Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ddd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axes[0].set_title('Model predictions')\n",
    "axes[0].scatter(\n",
    "    testing_df[label], predictions,\n",
    "    c='black', s=0.5, alpha=0.5\n",
    ")\n",
    "axes[0].plot(\n",
    "    [testing_df[label].min(), testing_df[label].max()],\n",
    "    [testing_df[label].min(), testing_df[label].max()],\n",
    "    color='red', linestyle='--'\n",
    ")\n",
    "axes[0].set_xlabel('True values (standardized)')\n",
    "axes[0].set_ylabel('Predicted values (standardized)')\n",
    "\n",
    "axes[1].set_title('Residuals vs predicted values')\n",
    "axes[1].scatter(\n",
    "    predictions, testing_df[label] - predictions,\n",
    "    c='black', s=0.5, alpha=0.5\n",
    ")\n",
    "axes[1].axhline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted values (standardized)')\n",
    "axes[1].set_ylabel('Residuals (standardized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
