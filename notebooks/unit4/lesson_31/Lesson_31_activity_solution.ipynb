{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8e9454",
   "metadata": {},
   "source": [
    "# PyTorch CNN Activity: Experimenting with Model Architecture and Data\n",
    "\n",
    "In this activity, you will experiment with different aspects of the CNN model from the Lesson 31 demo to understand how architectural choices and data preprocessing affect model performance.\n",
    "\n",
    "## Activity Overview\n",
    "\n",
    "You will conduct **three experiments** using the code provided in the demo notebook:\n",
    "\n",
    "1. **Experiment 1**: Add more convolutional blocks and/or modify filters and filter sizes\n",
    "2. **Experiment 2**: Use RGB images instead of grayscale\n",
    "3. **Experiment 3**: Add image augmentation using PyTorch transforms\n",
    "\n",
    "For each experiment, you will:\n",
    "- Modify the relevant code sections\n",
    "- Train the model\n",
    "- Compare results with the baseline model\n",
    "- Document your observations\n",
    "\n",
    "**Note**: You may want to reduce the number of epochs (e.g., to 20-30) to speed up experimentation.\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd302c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc, \n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(315)\n",
    "np.random.seed(315)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bb71a",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8b361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000 # Training images come in 5 batches of 10,000\n",
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "print_every = 5 # Print training progress every n epochs\n",
    "\n",
    "# CIFAR-10 class names in class order\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc030588",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 1: Modify Convolutional Architecture\n",
    "\n",
    "**Objective**: Explore how adding more convolutional blocks or changing filter counts and kernel sizes affects model performance.\n",
    "\n",
    "### 1.1. Load and Preprocess Data (Baseline - Grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da77825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Make sure data directory exists\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data preprocessing: convert to grayscale, tensor, and normalize\n",
    "transform_exp1 = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset_exp1 = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_exp1\n",
    ")\n",
    "\n",
    "test_dataset_exp1 = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_exp1\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset_exp1)}')\n",
    "print(f'Test samples: {len(test_dataset_exp1)}')\n",
    "print(f'Image shape: {train_dataset_exp1[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6f135",
   "metadata": {},
   "source": [
    "### 1.2. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ff48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 40\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation and testing tensors\n",
    "X_train_full = torch.stack([img for img, _ in train_dataset_exp1]).to(device)\n",
    "y_train_full = torch.tensor([label for _, label in train_dataset_exp1]).to(device)\n",
    "\n",
    "X_test_exp1 = torch.stack([img for img, _ in test_dataset_exp1]).to(device)\n",
    "y_test_exp1 = torch.tensor([label for _, label in test_dataset_exp1]).to(device)\n",
    "\n",
    "# Split training data into train and validation sets (80/20 split)\n",
    "n_train = int(0.8 * len(X_train_full))\n",
    "indices = torch.randperm(len(X_train_full))\n",
    "\n",
    "X_train_exp1 = X_train_full[indices[:n_train]]\n",
    "y_train_exp1 = y_train_full[indices[:n_train]]\n",
    "X_val_exp1 = X_train_full[indices[n_train:]]\n",
    "y_val_exp1 = y_train_full[indices[n_train:]]\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_tensor_dataset_exp1 = torch.utils.data.TensorDataset(X_train_exp1, y_train_exp1)\n",
    "val_tensor_dataset_exp1 = torch.utils.data.TensorDataset(X_val_exp1, y_val_exp1)\n",
    "test_tensor_dataset_exp1 = torch.utils.data.TensorDataset(X_test_exp1, y_test_exp1)\n",
    "\n",
    "train_loader_exp1 = DataLoader(train_tensor_dataset_exp1, batch_size=batch_size, shuffle=True)\n",
    "val_loader_exp1 = DataLoader(val_tensor_dataset_exp1, batch_size=batch_size, shuffle=False)\n",
    "test_loader_exp1 = DataLoader(test_tensor_dataset_exp1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Training batches: {len(train_loader_exp1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926ebcb",
   "metadata": {},
   "source": [
    "### 1.3. Define Modified CNN Architecture\n",
    "\n",
    "**TODO**: Modify the model architecture below. Try one or more of these changes:\n",
    "\n",
    "- Add another convolutional block (Conv2d -> BatchNorm2d -> ReLU -> Conv2d -> BatchNorm2d -> ReLU -> MaxPool2d -> Dropout)\n",
    "- Increase the number of filters (e.g., change 32 to 64 or 128)\n",
    "- Experiment with different kernel sizes (e.g., 5x5 instead of 3x3)\n",
    "- Try different pooling strategies\n",
    "\n",
    "**Important**: Remember to update the input size to the first Linear layer if you change the architecture! The size depends on:\n",
    "- Number of filters in the last conv layer\n",
    "- Final spatial dimensions after pooling\n",
    "\n",
    "*Hint*: For a 32x32 image, each MaxPool2d(2,2) layer divides dimensions by 2. So:\n",
    "- After 1 pooling: 32 → 16\n",
    "- After 2 poolings: 32 → 16 → 8\n",
    "- After 3 poolings: 32 → 16 → 8 → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e834bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Dropout(p=0.25, inplace=False)\n",
      "  (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU()\n",
      "  (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): ReLU()\n",
      "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (15): Dropout(p=0.25, inplace=False)\n",
      "  (16): Flatten(start_dim=1, end_dim=-1)\n",
      "  (17): Linear(in_features=8192, out_features=256, bias=True)\n",
      "  (18): ReLU()\n",
      "  (19): Dropout(p=0.5, inplace=False)\n",
      "  (20): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 2359754\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model_exp1 = nn.Sequential(\n",
    "\n",
    "    # Conv block 1: grayscale input -> 64 filters\n",
    "    nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.25),\n",
    "    \n",
    "    # Conv block 2: 64 -> 128 filters\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.25),\n",
    "    \n",
    "    # Classifier\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128 * 8 * 8, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes)\n",
    "\n",
    ").to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_exp1.parameters() if p.requires_grad)\n",
    "print(model_exp1)\n",
    "print(f'\\nTotal parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c435f3",
   "metadata": {},
   "source": [
    "### 1.4. Train Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4102e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epochs: int = 10,\n",
    "    print_every: int = 1,\n",
    "    device: torch.device = None\n",
    ") -> dict[str, list[float]]:\n",
    "    '''Training loop for PyTorch classification model.\n",
    "    \n",
    "    Args:\n",
    "        device: If provided, moves batches to this device on-the-fly.\n",
    "                If None, assumes data is already on the correct device.\n",
    "    '''\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            \n",
    "            # Move batch to device if specified\n",
    "            if device is not None:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images, labels in val_loader:\n",
    "                \n",
    "                # Move batch to device if specified\n",
    "                if device is not None:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        # Record metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "\n",
    "            print(\n",
    "                f'Epoch {epoch+1}/{epochs} - ' +\n",
    "                f'loss: {train_loss:.4f} - ' +\n",
    "                f'accuracy: {train_accuracy:.2f}% - ' +\n",
    "                f'val_loss: {val_loss:.4f} - ' +\n",
    "                f'val_accuracy: {val_accuracy:.2f}%'\n",
    "            )\n",
    "\n",
    "    print('\\nTraining complete.')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865041b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB. GPU 1 has a total capacity of 7.92 GiB of which 150.12 MiB is free. Process 3997982 has 5.01 GiB memory in use. Including non-PyTorch memory, this process has 1.82 GiB memory in use. Of the allocated memory 1.61 GiB is allocated by PyTorch, and 120.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, print_every, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:253\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:553\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    537\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    538\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    546\u001b[0m     )\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 1 has a total capacity of 7.92 GiB of which 150.12 MiB is free. Process 3997982 has 5.01 GiB memory in use. Including non-PyTorch memory, this process has 1.82 GiB memory in use. Of the allocated memory 1.61 GiB is allocated by PyTorch, and 120.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "criterion_exp1 = nn.CrossEntropyLoss()\n",
    "optimizer_exp1 = optim.Adam(model_exp1.parameters(), lr=learning_rate)\n",
    "\n",
    "history_exp1 = train_model(\n",
    "    model=model_exp1,\n",
    "    train_loader=train_loader_exp1,\n",
    "    val_loader=val_loader_exp1,\n",
    "    criterion=criterion_exp1,\n",
    "    optimizer=optimizer_exp1,\n",
    "    epochs=epochs,\n",
    "    print_every=print_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdf140",
   "metadata": {},
   "source": [
    "### 1.5. Evaluate and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08942db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device = None\n",
    ") -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    '''Evaluate model on test set.\n",
    "    \n",
    "    Args:\n",
    "        device: If provided, moves batches to this device on-the-fly.\n",
    "                If None, assumes data is already on the correct device.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            \n",
    "            # Move batch to device if specified\n",
    "            if device is not None:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, np.array(all_predictions), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0afcf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_exp1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      4\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss - Experiment 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory_exp1\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(history_exp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_exp1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].set_title('Loss - Experiment 1')\n",
    "axes[0].plot(history_exp1['train_loss'], label='Train')\n",
    "axes[0].plot(history_exp1['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (cross-entropy)')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].set_title('Accuracy - Experiment 1')\n",
    "axes[1].plot(history_exp1['train_accuracy'], label='Train')\n",
    "axes[1].plot(history_exp1['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy_exp1, predictions_exp1, true_labels_exp1 = evaluate_model(model_exp1, test_loader_exp1)\n",
    "print(f'\\nExperiment 1 Test Accuracy: {test_accuracy_exp1:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d510de",
   "metadata": {},
   "source": [
    "### 1.6. Experiment 1 Observations\n",
    "\n",
    "**TODO**: Document your findings:\n",
    "- What architectural changes did you make?\n",
    "- How did these changes affect the number of parameters?\n",
    "- Did the model perform better or worse than the baseline?\n",
    "- What did you observe about training time and convergence?\n",
    "- Did you notice any overfitting or underfitting?\n",
    "\n",
    "*Your notes here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc07230",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 2: Use RGB Images Instead of Grayscale\n",
    "\n",
    "**Objective**: Compare model performance using full RGB color information versus grayscale.\n",
    "\n",
    "### 2.1. Load and Preprocess RGB Data\n",
    "\n",
    "**TODO**: Modify the transform to keep RGB channels (3 channels) instead of converting to grayscale. Update the normalization to use 3 mean and std values.\n",
    "\n",
    "*Hint*: For RGB, use `transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf37587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify this transform to use RGB instead of grayscale\n",
    "transform_exp2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load training and test datasets with RGB\n",
    "train_dataset_exp2 = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_exp2\n",
    ")\n",
    "\n",
    "test_dataset_exp2 = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_exp2\n",
    ")\n",
    "\n",
    "print(f'Image shape: {train_dataset_exp2[0][0].shape}')  # Should be [3, 32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ac7b8",
   "metadata": {},
   "source": [
    "### 2.2. Visualize RGB Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72836e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 10 RGB images from the training dataset\n",
    "ncols = 5\n",
    "nrows = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*1.5, nrows*1.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset_exp2[i]\n",
    "    \n",
    "    # Unnormalize and transpose for plotting\n",
    "    img = img * 0.5 + 0.5\n",
    "    img = img.numpy().transpose(1, 2, 0)  # Change from CxHxW to HxWxC\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da306112",
   "metadata": {},
   "source": [
    "### 2.3. Create Data Loaders for RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation and testing tensors\n",
    "X_train_full_exp2 = torch.stack([img for img, _ in train_dataset_exp2]).to(device)\n",
    "y_train_full_exp2 = torch.tensor([label for _, label in train_dataset_exp2]).to(device)\n",
    "\n",
    "X_test_exp2 = torch.stack([img for img, _ in test_dataset_exp2]).to(device)\n",
    "y_test_exp2 = torch.tensor([label for _, label in test_dataset_exp2]).to(device)\n",
    "\n",
    "# Split training data\n",
    "n_train = int(0.8 * len(X_train_full_exp2))\n",
    "indices = torch.randperm(len(X_train_full_exp2))\n",
    "\n",
    "X_train_exp2 = X_train_full_exp2[indices[:n_train]]\n",
    "y_train_exp2 = y_train_full_exp2[indices[:n_train]]\n",
    "X_val_exp2 = X_train_full_exp2[indices[n_train:]]\n",
    "y_val_exp2 = y_train_full_exp2[indices[n_train:]]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_tensor_dataset_exp2 = torch.utils.data.TensorDataset(X_train_exp2, y_train_exp2)\n",
    "val_tensor_dataset_exp2 = torch.utils.data.TensorDataset(X_val_exp2, y_val_exp2)\n",
    "test_tensor_dataset_exp2 = torch.utils.data.TensorDataset(X_test_exp2, y_test_exp2)\n",
    "\n",
    "train_loader_exp2 = DataLoader(train_tensor_dataset_exp2, batch_size=batch_size, shuffle=True)\n",
    "val_loader_exp2 = DataLoader(val_tensor_dataset_exp2, batch_size=batch_size, shuffle=False)\n",
    "test_loader_exp2 = DataLoader(test_tensor_dataset_exp2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'X_train shape: {X_train_exp2.shape}')  # Should show 3 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a5f59",
   "metadata": {},
   "source": [
    "### 2.4. Define CNN for RGB Images\n",
    "\n",
    "**TODO**: Modify the first Conv2d layer to accept 3 input channels instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadebd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated first conv layer to accept 3 channels (RGB)\n",
    "model_exp2 = nn.Sequential(\n",
    "\n",
    "    # Conv block: RGB input (3 channels)\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.5),\n",
    "    \n",
    "    # Classifier\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 16 * 16, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, num_classes)\n",
    "\n",
    ").to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_exp2.parameters() if p.requires_grad)\n",
    "print(model_exp2)\n",
    "print(f'\\nTotal parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f01b3",
   "metadata": {},
   "source": [
    "### 2.5. Train RGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6104f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "criterion_exp2 = nn.CrossEntropyLoss()\n",
    "optimizer_exp2 = optim.Adam(model_exp2.parameters(), lr=learning_rate)\n",
    "\n",
    "history_exp2 = train_model(\n",
    "    model=model_exp2,\n",
    "    train_loader=train_loader_exp2,\n",
    "    val_loader=val_loader_exp2,\n",
    "    criterion=criterion_exp2,\n",
    "    optimizer=optimizer_exp2,\n",
    "    epochs=epochs,\n",
    "    print_every=print_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea58e7",
   "metadata": {},
   "source": [
    "### 2.6. Evaluate RGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed85bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].set_title('Loss - Experiment 2 (RGB)')\n",
    "axes[0].plot(history_exp2['train_loss'], label='Train')\n",
    "axes[0].plot(history_exp2['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (cross-entropy)')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].set_title('Accuracy - Experiment 2 (RGB)')\n",
    "axes[1].plot(history_exp2['train_accuracy'], label='Train')\n",
    "axes[1].plot(history_exp2['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy_exp2, predictions_exp2, true_labels_exp2 = evaluate_model(model_exp2, test_loader_exp2)\n",
    "print(f'\\nExperiment 2 Test Accuracy: {test_accuracy_exp2:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a099e",
   "metadata": {},
   "source": [
    "### 2.7. Experiment 2 Observations\n",
    "\n",
    "**TODO**: Document your findings:\n",
    "- Did RGB improve accuracy compared to grayscale?\n",
    "- How did the number of parameters change?\n",
    "- Which classes benefited most from color information?\n",
    "- Were there any classes that performed similarly with both grayscale and RGB?\n",
    "\n",
    "*Your notes here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ee0bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 3: Add Image Augmentation\n",
    "\n",
    "**Objective**: Use PyTorch transforms to augment training data and improve model generalization.\n",
    "\n",
    "### 3.1. Define Augmented Transforms\n",
    "\n",
    "**TODO**: Add image augmentation transforms to the training data. Consider:\n",
    "- `transforms.RandomHorizontalFlip(p=0.5)` - Randomly flip images horizontally\n",
    "- `transforms.RandomCrop(32, padding=4)` - Random crop with padding\n",
    "- `transforms.RandomRotation(degrees=15)` - Small random rotations\n",
    "- `transforms.ColorJitter(brightness=0.2, contrast=0.2)` - Random brightness/contrast adjustments\n",
    "\n",
    "**Note**: Apply augmentation only to training data, not validation or test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c96439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transform with augmentation\n",
    "transform_train_exp3 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Validation and test transforms (no augmentation)\n",
    "transform_test_exp3 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load datasets with augmentation\n",
    "train_dataset_exp3 = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train_exp3  # Augmented transform\n",
    ")\n",
    "\n",
    "test_dataset_exp3 = datasets.CIFAR10(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test_exp3  # No augmentation\n",
    ")\n",
    "\n",
    "print('Datasets loaded with augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43da7e",
   "metadata": {},
   "source": [
    "### 3.2. Visualize Augmented Images\n",
    "\n",
    "Run this cell multiple times to see different augmentations of the same images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented versions of the same images\n",
    "ncols = 5\n",
    "nrows = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*1.5, nrows*1.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Get augmented version each time this is run\n",
    "    img, label = train_dataset_exp3[i]\n",
    "    \n",
    "    # Unnormalize and transpose for plotting\n",
    "    img = img * 0.5 + 0.5\n",
    "    img = img.numpy().transpose(1, 2, 0)\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Augmented Training Images (run cell again to see different augmentations)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0683e5",
   "metadata": {},
   "source": [
    "### 3.3. Create Data Loaders with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data augmentation, we must NOT preload data to GPU as tensors.\n",
    "# Transforms need to be applied on-the-fly during each epoch so each \n",
    "# batch sees different augmented versions of the images.\n",
    "\n",
    "# Split training data into train and validation sets using Subset\n",
    "n_train = int(0.8 * len(train_dataset_exp3))\n",
    "n_val = len(train_dataset_exp3) - n_train\n",
    "indices = torch.randperm(len(train_dataset_exp3)).tolist()\n",
    "\n",
    "train_subset_exp3 = torch.utils.data.Subset(train_dataset_exp3, indices[:n_train])\n",
    "val_subset_exp3 = torch.utils.data.Subset(train_dataset_exp3, indices[n_train:])\n",
    "\n",
    "print(f'Training samples: {len(train_subset_exp3)}')\n",
    "print(f'Validation samples: {len(val_subset_exp3)}')\n",
    "print(f'Test samples: {len(test_dataset_exp3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders directly from Dataset/Subset objects\n",
    "# Transforms are applied on-the-fly when batches are loaded\n",
    "train_loader_exp3 = DataLoader(\n",
    "    train_subset_exp3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader_exp3 = DataLoader(\n",
    "    val_subset_exp3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader_exp3 = DataLoader(\n",
    "    test_dataset_exp3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training batches: {len(train_loader_exp3)}')\n",
    "print(f'Validation batches: {len(val_loader_exp3)}')\n",
    "print(f'Test batches: {len(test_loader_exp3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a28d8",
   "metadata": {},
   "source": [
    "### 3.4. Define Model for Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5365b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture as Experiment 2 (RGB)\n",
    "model_exp3 = nn.Sequential(\n",
    "\n",
    "    # Conv block: RGB input\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Dropout(0.5),\n",
    "    \n",
    "    # Classifier\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 16 * 16, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, num_classes)\n",
    "\n",
    ").to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_exp3.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d131b",
   "metadata": {},
   "source": [
    "### 3.5. Train Model with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "criterion_exp3 = nn.CrossEntropyLoss()\n",
    "optimizer_exp3 = optim.Adam(model_exp3.parameters(), lr=learning_rate)\n",
    "\n",
    "# Pass device to move batches on-the-fly (required for on-the-fly augmentation)\n",
    "history_exp3 = train_model(\n",
    "    model=model_exp3,\n",
    "    train_loader=train_loader_exp3,\n",
    "    val_loader=val_loader_exp3,\n",
    "    criterion=criterion_exp3,\n",
    "    optimizer=optimizer_exp3,\n",
    "    epochs=epochs,\n",
    "    print_every=print_every,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad8659",
   "metadata": {},
   "source": [
    "### 3.6. Evaluate Augmented Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].set_title('Loss - Experiment 3 (Augmented)')\n",
    "axes[0].plot(history_exp3['train_loss'], label='Train')\n",
    "axes[0].plot(history_exp3['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (cross-entropy)')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].set_title('Accuracy - Experiment 3 (Augmented)')\n",
    "axes[1].plot(history_exp3['train_accuracy'], label='Train')\n",
    "axes[1].plot(history_exp3['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test accuracy (pass device for on-the-fly batch loading)\n",
    "test_accuracy_exp3, predictions_exp3, true_labels_exp3 = evaluate_model(\n",
    "    model_exp3, test_loader_exp3, device=device\n",
    ")\n",
    "print(f'\\nExperiment 3 Test Accuracy: {test_accuracy_exp3:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd1142",
   "metadata": {},
   "source": [
    "### 3.7. Experiment 3 Observations\n",
    "\n",
    "**TODO**: Document your findings:\n",
    "- Which augmentation techniques did you use?\n",
    "- Did augmentation improve test accuracy?\n",
    "- Did you notice any effect on the gap between training and validation accuracy (overfitting)?\n",
    "- How did training time compare to non-augmented training?\n",
    "- Would you recommend augmentation for this dataset?\n",
    "\n",
    "*Your notes here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa6bc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Compare All Experiments\n",
    "\n",
    "### Compare Test Accuracies\n",
    "\n",
    "**TODO**: Fill in your results and compare:\n",
    "\n",
    "| Experiment | Description | Test Accuracy | Notes |\n",
    "|------------|-------------|---------------|-------|\n",
    "| Baseline (demo) | Grayscale, simple architecture | ~60% | From demo notebook |\n",
    "| Experiment 1 | Modified architecture | _% | |\n",
    "| Experiment 2 | RGB images | _% | |\n",
    "| Experiment 3 | Image augmentation | _% | |\n",
    "\n",
    "### Final Reflections\n",
    "\n",
    "**TODO**: Based on your experiments, answer these questions:\n",
    "\n",
    "1. Which experiment produced the best results and why do you think that is?\n",
    "\n",
    "2. What trade-offs did you observe between model complexity, training time, and performance?\n",
    "\n",
    "3. If you were to combine multiple improvements (e.g., deeper architecture + RGB + augmentation), what would you expect?\n",
    "\n",
    "4. What other experiments would you like to try?\n",
    "\n",
    "*Your reflections here:*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
