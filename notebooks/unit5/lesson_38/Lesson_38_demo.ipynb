{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e22d35",
   "metadata": {},
   "source": [
    "# Lesson 38: Text data analysis demonstration\n",
    "\n",
    "This notebook demonstrates key concepts and tools for text data analysis in NLP.\n",
    "\n",
    "**1. Text preprocessing**\n",
    "- Tokenization\n",
    "- Normalization and cleaning\n",
    "- Stemming and lemmatization\n",
    "\n",
    "**2. Text exploration**\n",
    "- Word frequency analysis\n",
    "- Word cloud visualization\n",
    "\n",
    "**3. Text classification**\n",
    "- Naive Bayes classifier\n",
    "\n",
    "**4. Rule-based sentiment analysis**\n",
    "- VADER sentiment analyzer\n",
    "\n",
    "\n",
    "## Notebook set up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aba526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, movie_reviews\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de835a",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use the NLTK movie reviews corpus, which contains 2000 movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie reviews corpus\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(documents, columns=['text', 'label'])\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Label distribution:\\n{df[\"label\"].value_counts()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80e430",
   "metadata": {},
   "source": [
    "## 1. Text preprocessing\n",
    "\n",
    "### 1.1. Tokenization\n",
    "\n",
    "Tokenization splits text into individual units (tokens) such as words or sentences.\n",
    "\n",
    "NLTK [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = df['text'].iloc[0][:500]\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(sample_text)\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_tokens = sent_tokenize(sample_text)\n",
    "\n",
    "print(f'Original text:\\n{sample_text}\\n')\n",
    "print(f'Word tokens ({len(word_tokens)} tokens):\\n{word_tokens[:20]}...\\n')\n",
    "print(f'Sentence tokens ({len(sent_tokens)} sentences):\\n{sent_tokens[:2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c1d8c",
   "metadata": {},
   "source": [
    "### 1.2. Normalization and cleaning\n",
    "\n",
    "Text normalization includes lowercasing, removing punctuation, and filtering stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "tokens_lower = [token.lower() for token in word_tokens]\n",
    "\n",
    "# Remove non-alphabetic tokens\n",
    "tokens_alpha = [token for token in tokens_lower if token.isalpha()]\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_clean = [token for token in tokens_alpha if token not in stop_words]\n",
    "\n",
    "print(f'Original tokens: {len(word_tokens)}')\n",
    "print(f'After lowercasing: {len(tokens_lower)}')\n",
    "print(f'After removing non-alpha: {len(tokens_alpha)}')\n",
    "print(f'After removing stopwords: {len(tokens_clean)}')\n",
    "print(f'\\nCleaned tokens:\\n{tokens_clean[:15]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d13aab",
   "metadata": {},
   "source": [
    "### 1.3. Stemming and lemmatization\n",
    "\n",
    "Stemming reduces words to their root form by removing suffixes. Lemmatization reduces words to their dictionary form (lemma).\n",
    "\n",
    "NLTK [PorterStemmer](https://www.nltk.org/api/nltk.stem.porter.html) and [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.wordnet.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "tokens_stemmed = [stemmer.stem(token) for token in tokens_clean]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lemmatized = [lemmatizer.lemmatize(token) for token in tokens_clean]\n",
    "\n",
    "# Compare results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'original': tokens_clean[:10],\n",
    "    'stemmed': tokens_stemmed[:10],\n",
    "    'lemmatized': tokens_lemmatized[:10]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e742076",
   "metadata": {},
   "source": [
    "## 2. Text exploration\n",
    "\n",
    "### 2.1. Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5930f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function for full corpus\n",
    "def preprocess_text(text):\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Get all tokens from corpus\n",
    "all_tokens = []\n",
    "\n",
    "for text in df['text']:\n",
    "    all_tokens.extend(preprocess_text(text))\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_tokens)\n",
    "top_20 = word_freq.most_common(20)\n",
    "\n",
    "# Display top words\n",
    "freq_df = pd.DataFrame(top_20, columns=['word', 'count'])\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0d820",
   "metadata": {},
   "source": [
    "### 2.2. Word cloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Greys'\n",
    ").generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Word cloud of movie reviews corpus')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9858a",
   "metadata": {},
   "source": [
    "## 3. Text classification\n",
    "\n",
    "### 3.1. Naive Bayes classifier\n",
    "\n",
    "Naive Bayes is a simple but effective classifier for text that uses word frequencies as features.\n",
    "\n",
    "Scikit-learn [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f78c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=315\n",
    ")\n",
    "\n",
    "# Vectorize text using bag-of-words\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f'Training set: {X_train_vec.shape}')\n",
    "print(f'Test set: {X_test_vec.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc23278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = nb_classifier.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.3f}\\n')\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5af326",
   "metadata": {},
   "source": [
    "## 4. Rule-based sentiment analysis\n",
    "\n",
    "### 4.1. VADER sentiment analyzer\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon-based sentiment analyzer that uses a dictionary of words with pre-assigned sentiment scores.\n",
    "\n",
    "NLTK [VADER](https://www.nltk.org/howto/sentiment.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example sentences\n",
    "example_texts = [\n",
    "    'This movie was absolutely fantastic! I loved every minute.',\n",
    "    'Terrible film. Complete waste of time and money.',\n",
    "    'The movie was okay. Nothing special but watchable.',\n",
    "    'I have mixed feelings about this one.'\n",
    "]\n",
    "\n",
    "# Analyze sentiment\n",
    "print('VADER sentiment scores:\\n')\n",
    "\n",
    "for text in example_texts:\n",
    "\n",
    "    scores = sia.polarity_scores(text)\n",
    "    print(f'Text: {text}')\n",
    "    print(f'Scores: {scores}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad42331",
   "metadata": {},
   "source": [
    "### 4.2. VADER evaluation on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VADER to test set\n",
    "def vader_predict(text):\n",
    "\n",
    "    scores = sia.polarity_scores(text)\n",
    "    if scores['compound'] >= 0.05:\n",
    "        return 'pos'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'neg'  # Default to negative for neutral\n",
    "\n",
    "# Predict with VADER\n",
    "y_pred_vader = [vader_predict(text) for text in X_test]\n",
    "vader_accuracy = accuracy_score(y_test, y_pred_vader)\n",
    "\n",
    "print(f'VADER accuracy: {vader_accuracy:.3f}')\n",
    "print(f'Naive Bayes accuracy: {accuracy:.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
